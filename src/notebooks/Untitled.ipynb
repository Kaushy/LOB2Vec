{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: module: line 1: syntax error: unexpected end of file\n",
      "/bin/sh: error importing function definition for `module'\n",
      "/bin/sh: scl: line 1: syntax error: unexpected end of file\n",
      "/bin/sh: error importing function definition for `scl'\n",
      "/bin/sh: scl: line 1: syntax error: unexpected end of file\n",
      "/bin/sh: error importing function definition for `scl'\n",
      "/bin/sh: module: line 1: syntax error: unexpected end of file\n",
      "/bin/sh: error importing function definition for `module'\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 8.0 MB/s \n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 3.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (2.23.0)\n",
      "Collecting attrs>=18.1.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: wrapt in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: numpy in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (1.18.4)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 17.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: absl-py in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (3.12.0)\n",
      "Requirement already satisfied: promise in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: termcolor in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.23.0-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /rds/general/user/kk2219/home/anaconda3/envs/spoofing_new/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow_datasets) (46.2.0.post20200511)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 2.4 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: dill, future\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=361ed2308fdabcf99e980d5bd0046b17d5dd7bd26c224c326eb0538d8784d08c\n",
      "  Stored in directory: /rds/general/user/kk2219/home/.cache/pip/wheels/72/6b/d5/5548aa1b73b8c3d176ea13f9f92066b02e82141549d90e2100\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=50b0956a8a567f0c3fa91094823b7dbbba28185684c2f5e04afd3b7c115cd0c8\n",
      "  Stored in directory: /rds/general/user/kk2219/home/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built dill future\n",
      "\u001b[31mERROR: tensorflow-metadata 0.23.0 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tqdm, attrs, dill, future, googleapis-common-protos, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed attrs-19.3.0 dill-0.3.2 future-0.18.2 googleapis-common-protos-1.52.0 tensorflow-datasets-3.2.1 tensorflow-metadata-0.23.0 tqdm-4.48.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from datetime import datetime,timedelta\n",
    "#import wandb\n",
    "#from GenericTools import *\n",
    "#from TripletLossTools import *\n",
    "#from QuadrupletLossTools import *\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Allow memory growth for the GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSetToArrays(nb_classes,datasets, splitname):\n",
    "    '''\n",
    "    Organise the tensorflow dataset into numpy arrays\n",
    "    Inputs\n",
    "        nb_classes : global number of classes\n",
    "        datasets : tensorflow datasets \"pack\" as they are in the tensorflow_datasets repo\n",
    "        splitname : which split name from the tfds to take\n",
    "    Output\n",
    "        dataset : list of real_nb_class element, each containing numpy array images for each class\n",
    "        dataset_flat_X : numpy array images of all classes\n",
    "        dataset_flat_Y : numpy array true labels of all classes\n",
    "        real_nb_class : number of class detected in this split\n",
    "    '''\n",
    "    dataset = []\n",
    "    tempdataset = []\n",
    "    dataset_flat_X = []\n",
    "    dataset_flat_Y = []\n",
    "    real_nb_class = 0\n",
    "    \n",
    "    for n in range(nb_classes):\n",
    "        tempdataset.append([])\n",
    "    \n",
    "    for row in tfds.as_numpy(datasets[splitname]):\n",
    "       \n",
    "        \n",
    "        #Add in the flat dataset\n",
    "        dataset_flat_X.append(row['image'])\n",
    "        dataset_flat_Y.append(row['label'])\n",
    "                \n",
    "        #Add in the sorted dataset\n",
    "        tempdataset[row['label']].append(row['image'])\n",
    "        \n",
    "    \n",
    "    #Transform into nparrays, inverted grayscale and normalize\n",
    "    for n in range(nb_classes):\n",
    "        if len(tempdataset[n])>1:\n",
    "            dataset.append(np.mean(np.asarray(tempdataset[n]), axis=-1, keepdims=True)/-255+1)\n",
    "            real_nb_class += 1\n",
    "    \n",
    "    #idem\n",
    "    dataset_flat_X = np.mean(np.asarray(dataset_flat_X), axis=-1, keepdims=True)/-255+1\n",
    "    dataset_flat_Y = np.asarray(dataset_flat_Y)\n",
    "    \n",
    "    return dataset,dataset_flat_X,dataset_flat_Y,real_nb_class\n",
    "\n",
    "def buildDataSet():\n",
    "    omniglot = tfds.builder('omniglot')\n",
    "    input_shape = omniglot.info.features['image'].shape\n",
    "    nb_classes = omniglot.info.features['label'].num_classes\n",
    "    omniglot.download_and_prepare()\n",
    "    datasets = omniglot.as_dataset()\n",
    "    \n",
    "    #Dataset sorted by class\n",
    "    dataset_train,dataset_train_flat_X,dataset_train_flat_Y,nb_classes_train = dataSetToArrays(nb_classes,datasets,'train')\n",
    "    dataset_test, dataset_test_flat_X, dataset_test_flat_Y,nb_classes_test  = dataSetToArrays(nb_classes,datasets,'test')\n",
    "    \n",
    "    return dataset_train,dataset_test, \\\n",
    "            dataset_train_flat_X,dataset_train_flat_Y,dataset_test_flat_X,dataset_test_flat_Y, \\\n",
    "            nb_classes,nb_classes_train,nb_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'op' and 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mtry_reraise\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, **builder_init_kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m       prefix=\"Failed to construct dataset {}\".format(name)):\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, config, version)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use the code version (do not restore data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_from_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_info.py\u001b[0m in \u001b[0;36minitialize_from_bucket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mtmp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcs_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs_dataset_info_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\u001b[0m in \u001b[0;36mgcs_dataset_info_files\u001b[0;34m(dataset_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;34m\"\"\"Return paths to GCS files in the given dataset directory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgcs_listdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCS_DATASET_INFO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\u001b[0m in \u001b[0;36mgcs_listdir\u001b[0;34m(dir_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mis_gcs_disabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mfile_exists_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error executing an HTTP request: libcurl code 77 meaning 'Problem with the SSL CA cert (path? access rights?)', error details: error setting certificate verify locations:\n  CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: none\n\t when reading metadata of gs://tfds-data/dataset_info/omniglot/3.0.0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-44ce6cd4cce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset_train_flat_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_train_flat_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_test_flat_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_test_flat_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_classes_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7e33c266a31c>\u001b[0m in \u001b[0;36mbuildDataSet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuildDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0momniglot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'omniglot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momniglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momniglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, **builder_init_kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m   with py_utils.try_reraise(\n\u001b[1;32m    243\u001b[0m       prefix=\"Failed to construct dataset {}\".format(name)):\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mtry_reraise\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(prefix, suffix)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'op' and 'message'"
     ]
    }
   ],
   "source": [
    "dataset_train,dataset_test, \\\n",
    "    dataset_train_flat_X,dataset_train_flat_Y,dataset_test_flat_X,dataset_test_flat_Y, \\\n",
    "    nb_classes, nb_classes_train,nb_classes_test = buildDataSet()\n",
    "img_rows, img_cols = dataset_train[0].shape[1],dataset_train[0].shape[2]\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(\"{} classes, {} for training, {} for testing\".format(nb_classes, nb_classes_train,nb_classes_test))\n",
    "print(\"Checking shapes for class 0 (train) : \",dataset_train[0].shape)\n",
    "print(\"Checking shapes for class 0 (test) : \",dataset_test[0].shape)\n",
    "print(\"Checking shape for flat train dataset X and Y\", dataset_train_flat_X.shape,dataset_train_flat_Y.shape)\n",
    "print(\"Checking shape for flat test  dataset X and Y\", dataset_test_flat_X.shape,dataset_test_flat_Y.shape)\n",
    "print(\"Checking first samples\")\n",
    "for i in range(2):\n",
    "    DrawPics(dataset_train[i],5,template='Train {}',classnumber=i)\n",
    "    DrawPics(dataset_test[i],5,template='Test {}',classnumber=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spoofing_new]",
   "language": "python",
   "name": "conda-env-spoofing_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
