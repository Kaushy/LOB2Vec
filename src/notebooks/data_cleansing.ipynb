{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/user/kk2219/home/LOB2Vec/data/full_depth/test_dev/\n",
      "/rds/general/user/kk2219/home/LOB2Vec/data/full_depth/test_dev/\n",
      "/rds/general/user/kk2219/home/LOB2Vec/data/full_depth/test_dev/20160912\n",
      "/rds/general/user/kk2219/home/LOB2Vec/data/full_depth/test_dev/20160912/ABEO_EDGX.npy\n",
      "(812, 30, 2, 3)\n",
      "/rds/general/user/kk2219/home/LOB2Vec/data/full_depth/test_dev/20160912/ABEO_EDGA.npy\n",
      "(461, 30, 2, 3)\n",
      "/rds/general/user/kk2219/home/LOB2Vec/data/full_depth/test_dev/20160912/ABEO_NASDAQ.npy\n",
      "(3257, 30, 2, 3)\n",
      "Written To /rds/general/user/kk2219/home/LOB2Vec/data/stacked/test_dev/500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Setting Paths\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "project_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import paths\n",
    "import model\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from pylab import *\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "# Parameters\n",
    "num_frames = 500\n",
    "\n",
    "def retrieve_cleansed_data(lob, width, filename):\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0,50))\n",
    "    quantile_transformer = QuantileTransformer()\n",
    "    \n",
    "    # As evidenced by above, we can technically select all in the second axis as there is only 1 element. However, \n",
    "    # because we need a 2d input we make it 0. The 3rd axis is side so we need this\n",
    "    lob_qty_buy = pd.DataFrame(lob['quantity'][:,0,0,0:20])\n",
    "    lob_qty_buy = lob_qty_buy.replace(0, np.NaN)\n",
    "    \n",
    "    lob_qty_sell = pd.DataFrame(lob['quantity'][:,0,1,0:20])\n",
    "    lob_qty_sell = lob_qty_sell.replace(0, np.NaN)\n",
    "  \n",
    "    lob_n, d, w, h = lob['quantity'].shape\n",
    "    b_qty = lob['quantity'][:,0,0,:]\n",
    "    s_qty = lob['quantity'][:,0,1,:]\n",
    "    lob_qty = np.stack((b_qty, s_qty), axis=2)\n",
    "\n",
    "    lob_qty = lob_qty.reshape(-1,1)\n",
    "    lob_qty = min_max_scaler.fit_transform(lob_qty)\n",
    "    lob_qty = lob_qty.reshape(lob_n, h, w)\n",
    "    \n",
    "    b_price = lob['price'][:,0,0,:]\n",
    "    s_price = lob['price'][:,0,1,:]\n",
    "    lob_price = np.stack((b_price, s_price), axis=2)\n",
    "\n",
    "    lob_price = lob_price.reshape(-1,1)\n",
    "    lob_price = min_max_scaler.fit_transform(lob_price)\n",
    "    lob_price = lob_price.reshape(lob_n, h, w)\n",
    "    \n",
    "    lob_states = np.dstack((lob_qty, lob_price))\n",
    "    lob_states = lob_states.reshape(lob_n, h, w, 2)\n",
    "\n",
    "    # We use the num_frames for step count so that the windows are non-overlapping. We can also use view_as_blocks but the issue with this is that it \n",
    "    # requires precise block splits. i.e: If block does not have enough data it will not make block\n",
    "    print(lob_states.shape)\n",
    "    if ((len(lob_states) - num_frames) < 0):\n",
    "        return [], []\n",
    "    else:\n",
    "        lob_states = view_as_windows(lob_states,(width,1,1,1), step=(num_frames,1,1,1))[...,0,0,0].transpose(0,4,1,2,3)\n",
    "        labels = np.full(len(lob_states), filename)\n",
    "        return lob_states, labels\n",
    "\n",
    "\n",
    "def convert_data_to_labels(data_source, frames):\n",
    "    X = None\n",
    "    Y = None\n",
    "    print(data_source)\n",
    "    for subdir, dirs, files in os.walk(data_source):\n",
    "        print(subdir)\n",
    "        for file in files:\n",
    "            data_path = os.path.join(subdir, file)\n",
    "            print(data_path)\n",
    "            npy = np.load(data_path)\n",
    "            x, y = retrieve_cleansed_data(npy, frames, file)\n",
    "            if len(x) > 0:\n",
    "                if X is not None:\n",
    "                    X = np.append(X, x, axis=0)\n",
    "                else:\n",
    "                    X = x\n",
    "                    \n",
    "            if len(y) > 0:    \n",
    "                if Y is not None:\n",
    "                    Y = np.append(Y, y, axis=0)\n",
    "                else:\n",
    "                    Y = y\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def save_data(data_source, data_dest, datatype):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    X, Y = convert_data_to_labels(data_source, num_frames)\n",
    "    np.save(data_dest + str(num_frames) + datatype + 'X.npy', X)\n",
    "    np.save(data_dest + str(num_frames) + datatype + 'Y.npy', Y)\n",
    "    print('Written To ' + str(data_dest + str(num_frames)))\n",
    "\n",
    "# To run this you need high memory machine\n",
    "def save_individual_files(data_source, save_location, frames):\n",
    "    if not os.path.exists(str(save_location) + str(frames) + '_X/'):\n",
    "        os.makedirs(str(save_location) + str(frames) + '_X/')\n",
    "    X, y = convert_data_to_labels(data_source, frames)\n",
    "    {np.save(save_location + str(frames) + '_X/' + str(k) + '.npy', v) for k, v in enumerate(X)}\n",
    "    Y_numeric = [model.label_dict[v] for v in y]\n",
    "    np.save(save_location + str(frames) + '_Y.npy', Y_numeric)\n",
    "    print('Written To ' + str(save_location) + str(frames))\n",
    "\n",
    "# Test Data\n",
    "#save_data(paths.source_test_2017, paths.dest_2017, '_Test2017_')\n",
    "\n",
    "#save_individual_files(paths.source_train_dev, paths.generator_train_dev, num_frames)\n",
    "#save_individual_files(paths.source_val_dev, paths.generator_val_dev, num_frames)\n",
    "save_individual_files(paths.source_test_dev, paths.generator_test_dev, num_frames)\n",
    "\n",
    "#save_individual_files(paths.source_train_2016, paths.generator_train_2016, num_frames)\n",
    "#save_individual_files(paths.source_val_2016, paths.generator_val_2016, num_frames)\n",
    "#save_individual_files(paths.source_test_2016, paths.generator_test_2016, num_frames)\n",
    "\n",
    "#save_individual_files(paths.source_train_2017, paths.generator_train_2017, num_frames)\n",
    "#save_individual_files(paths.source_val_2017, paths.generator_val_2017, num_frames)\n",
    "#save_individual_files(paths.source_test_2017, paths.generator_test_2017, num_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths.dest_2016_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy = np.load('/rds/general/user/kk2219/ephemeral/data/lob_2_vec/full_depth/train_2016/20160902/ABEO_ARCA.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = np.zeros(1, dtype=[('timestamp', ('<M8[us]', [2, 20]))])\n",
    "npy['timestamp'][:,0,0,:][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy\n",
    "x = numpy.diff(npy['timestamp'][:,0,0,:], axis=0, prepend=npy['timestamp'][:,0,0,:][0].reshape(1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy['timestamp'][:,0,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy['timestamp'][:,0,0,:][0].reshape(1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x / np.timedelta64(1, 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "X = [[ 1., -1.,  2.5],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "transformer = MaxAbsScaler().fit(X)\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.load(paths.dest__dev_generator + 'X/' + str(4) + '.npy')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spoofing_new]",
   "language": "python",
   "name": "conda-env-spoofing_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
