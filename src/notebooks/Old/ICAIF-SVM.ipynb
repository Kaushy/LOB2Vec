{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from skimage.util.shape import view_as_windows\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Layer, Lambda, Flatten, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input, Model, metrics\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "project_path = os.path.abspath(os.path.join('../..'))\n",
    "train_source = os.path.abspath(os.path.join('../../data/train_source/'))\n",
    "test_source = os.path.abspath(os.path.join('../../data/test_source/'))\n",
    "train_path = os.path.abspath(os.path.join('../../data/train/'))\n",
    "test_path = os.path.abspath(os.path.join('../../data/test/'))\n",
    "dest_path_bsu = os.path.abspath(os.path.join('../../data/final_bsu/'))\n",
    "dest_path_bs = os.path.abspath(os.path.join('../../data/final_bs/'))\n",
    "val_path_bs = os.path.abspath(os.path.join('../../data/validation_bs/'))\n",
    "val_path_bsu = os.path.abspath(os.path.join('../../data/validation_bsu/'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "    \n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_classes=2\n",
    "cols, rows = 2, 30\n",
    "input_shape = (cols, rows, 1)\n",
    "vol_imb_diff = 0.1\n",
    "evaluate_every = 100 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 20000 # No. of training iterations\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "h = 30\n",
    "w = 2\n",
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265825, 10, 30, 2, 2)\n",
      "(93741, 10, 30, 2, 2)\n",
      "(19733, 10, 30, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "num_frames = 10\n",
    "# FOR BS Training \n",
    "# Train\n",
    "X_train = np.load(dest_path_bs + '/' + str(num_frames) + '_X_train.npy')\n",
    "Y_train = np.load(dest_path_bs + '/' + str(num_frames) + '_Y_train.npy')\n",
    "F_train = np.load(dest_path_bs + '/' + str(num_frames) + '_F_train.npy')\n",
    "X_test = np.load(dest_path_bs + '/' + str(num_frames) + '_X_test.npy')\n",
    "Y_test = np.load(dest_path_bs + '/' + str(num_frames) + '_Y_test.npy')\n",
    "F_test = np.load(dest_path_bs + '/' + str(num_frames) + '_F_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#X_train = np.append(X_train, X_test, axis=0)\n",
    "#Y_train = np.append(Y_train, Y_test, axis=0)\n",
    "#F_train = np.append(F_train, F_test, axis=0)\n",
    "#print(X_train.shape)\n",
    "\n",
    "# Validation\n",
    "X_val = np.load(val_path_bs + '/' + str(num_frames) + '_X_val.npy')\n",
    "Y_val = np.load(val_path_bs + '/' + str(num_frames) + '_Y_val.npy')\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, batch_size,\n",
    "                 name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes    \n",
    "\n",
    "        self.cat_true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):     \n",
    "\n",
    "        y_true = K.argmax(y_true, axis=-1)\n",
    "        y_pred = K.argmax(y_pred, axis=-1)\n",
    "        y_true = K.flatten(y_true)\n",
    "\n",
    "        true_poss = K.sum(K.cast((K.equal(y_true, y_pred)), dtype=tf.float32))\n",
    "        self.cat_true_positives.assign_add(true_poss)\n",
    "\n",
    "    def result(self):\n",
    "        return self.cat_true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 4)                 2000      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 2,015\n",
      "Trainable params: 2,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.2923 - accuracy: 0.1742\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2832 - accuracy: 0.2205\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2763 - accuracy: 0.2762\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.2713 - accuracy: 0.3464\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2675 - accuracy: 0.4322\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2642 - accuracy: 0.4915\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2610 - accuracy: 0.5185\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.2578 - accuracy: 0.5340\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.2544 - accuracy: 0.5466\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2510 - accuracy: 0.5564\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2477 - accuracy: 0.5629\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2446 - accuracy: 0.5684\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2418 - accuracy: 0.5719\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2393 - accuracy: 0.5753\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2370 - accuracy: 0.5840\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2349 - accuracy: 0.5905\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2329 - accuracy: 0.5952\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2311 - accuracy: 0.6001\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2293 - accuracy: 0.6046\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2276 - accuracy: 0.6088\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2259 - accuracy: 0.6135\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2243 - accuracy: 0.6181\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2228 - accuracy: 0.6216\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2213 - accuracy: 0.6249\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2199 - accuracy: 0.6282\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2186 - accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2173 - accuracy: 0.6408\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2160 - accuracy: 0.6447\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2147 - accuracy: 0.6521\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2135 - accuracy: 0.6542\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2124 - accuracy: 0.6563\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2112 - accuracy: 0.6590\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2101 - accuracy: 0.6639\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2090 - accuracy: 0.6690\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2080 - accuracy: 0.6702\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2069 - accuracy: 0.6713\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2059 - accuracy: 0.6723\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.2049 - accuracy: 0.6733\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2040 - accuracy: 0.6745\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2030 - accuracy: 0.6756\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2021 - accuracy: 0.6762\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2012 - accuracy: 0.6772\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2003 - accuracy: 0.6783\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1994 - accuracy: 0.6795\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1985 - accuracy: 0.6806\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1977 - accuracy: 0.6818\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1968 - accuracy: 0.6843\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1960 - accuracy: 0.6854\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1952 - accuracy: 0.6864\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1944 - accuracy: 0.6873\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1936 - accuracy: 0.6883\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1928 - accuracy: 0.6892\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1920 - accuracy: 0.6903\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1912 - accuracy: 0.6914\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1904 - accuracy: 0.6928\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1897 - accuracy: 0.6937\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1889 - accuracy: 0.6945\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1882 - accuracy: 0.6955\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1874 - accuracy: 0.6965\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1867 - accuracy: 0.6976\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1859 - accuracy: 0.6986\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1852 - accuracy: 0.6993\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1845 - accuracy: 0.7001\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1838 - accuracy: 0.7010\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.1830 - accuracy: 0.7017\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1823 - accuracy: 0.7022\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1816 - accuracy: 0.7030\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1809 - accuracy: 0.7036\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.1802 - accuracy: 0.7043\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1796 - accuracy: 0.7052\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1789 - accuracy: 0.7063\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1782 - accuracy: 0.7068\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1776 - accuracy: 0.7075\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1770 - accuracy: 0.7082\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.1763 - accuracy: 0.7091\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 26s 3s/step - loss: 0.1757 - accuracy: 0.7099\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1751 - accuracy: 0.7107\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1745 - accuracy: 0.7115\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1739 - accuracy: 0.7121\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1734 - accuracy: 0.7127\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 25s 2s/step - loss: 0.1728 - accuracy: 0.7134\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1722 - accuracy: 0.7142\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1717 - accuracy: 0.7151\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1711 - accuracy: 0.7159\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1706 - accuracy: 0.7166\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1701 - accuracy: 0.7174\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1695 - accuracy: 0.7181\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1690 - accuracy: 0.7188\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1685 - accuracy: 0.7195\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1680 - accuracy: 0.7202\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.1676 - accuracy: 0.7210\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1671 - accuracy: 0.7217\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1666 - accuracy: 0.7224\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1661 - accuracy: 0.7232\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1657 - accuracy: 0.7237\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1652 - accuracy: 0.7244\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1648 - accuracy: 0.7250\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1644 - accuracy: 0.7254\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1639 - accuracy: 0.7261\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1635 - accuracy: 0.7267\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1631 - accuracy: 0.7273\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1627 - accuracy: 0.7278\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1622 - accuracy: 0.7285\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1618 - accuracy: 0.7291\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.1614 - accuracy: 0.7298\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 25s 2s/step - loss: 0.1610 - accuracy: 0.7306\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1606 - accuracy: 0.7311\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1602 - accuracy: 0.7317\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1598 - accuracy: 0.7322\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1595 - accuracy: 0.7328\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1591 - accuracy: 0.7334\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1587 - accuracy: 0.7340\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1583 - accuracy: 0.7348\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1579 - accuracy: 0.7353\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1576 - accuracy: 0.7358\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1572 - accuracy: 0.7365\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1568 - accuracy: 0.7370\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1564 - accuracy: 0.7376\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1561 - accuracy: 0.7380\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1557 - accuracy: 0.7386\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1554 - accuracy: 0.7391\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1550 - accuracy: 0.7395\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1547 - accuracy: 0.7399\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1544 - accuracy: 0.7404\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1540 - accuracy: 0.7409\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1537 - accuracy: 0.7414\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1534 - accuracy: 0.7418\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1530 - accuracy: 0.7422\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1527 - accuracy: 0.7426\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1524 - accuracy: 0.7430\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1521 - accuracy: 0.7438\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1518 - accuracy: 0.7447\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1515 - accuracy: 0.7454\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1512 - accuracy: 0.7458\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1509 - accuracy: 0.7463\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1506 - accuracy: 0.7471\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1502 - accuracy: 0.7483\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1499 - accuracy: 0.7489\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1496 - accuracy: 0.7496\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1493 - accuracy: 0.7502\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1491 - accuracy: 0.7506\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1488 - accuracy: 0.7511\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1485 - accuracy: 0.7516\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1482 - accuracy: 0.7520\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1479 - accuracy: 0.7525\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1476 - accuracy: 0.7530\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1473 - accuracy: 0.7535\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1470 - accuracy: 0.7540\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1468 - accuracy: 0.7544\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1465 - accuracy: 0.7548\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1462 - accuracy: 0.7554\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1460 - accuracy: 0.7562\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1457 - accuracy: 0.7569\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1454 - accuracy: 0.7573\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1452 - accuracy: 0.7576\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1449 - accuracy: 0.7580\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1446 - accuracy: 0.7585\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 25s 3s/step - loss: 0.1444 - accuracy: 0.7588\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1441 - accuracy: 0.7591\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 26s 3s/step - loss: 0.1439 - accuracy: 0.7596\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.1436 - accuracy: 0.7599\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1434 - accuracy: 0.7603\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1431 - accuracy: 0.7607\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 25s 2s/step - loss: 0.1429 - accuracy: 0.7612\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1426 - accuracy: 0.7617\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1424 - accuracy: 0.7620\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1422 - accuracy: 0.7624\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1419 - accuracy: 0.7626\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1417 - accuracy: 0.7629\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 25s 2s/step - loss: 0.1415 - accuracy: 0.7633\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1412 - accuracy: 0.7637\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1410 - accuracy: 0.7642\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.1408 - accuracy: 0.7646\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1405 - accuracy: 0.7649\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1403 - accuracy: 0.7652\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1401 - accuracy: 0.7656\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1399 - accuracy: 0.7658\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1396 - accuracy: 0.7661\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1394 - accuracy: 0.7664\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1392 - accuracy: 0.7668\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1390 - accuracy: 0.7671\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1388 - accuracy: 0.7675\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1385 - accuracy: 0.7679\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1383 - accuracy: 0.7682\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1381 - accuracy: 0.7685\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1379 - accuracy: 0.7687\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1377 - accuracy: 0.7690\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1375 - accuracy: 0.7693\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1373 - accuracy: 0.7695\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1371 - accuracy: 0.7699\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1369 - accuracy: 0.7702\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1367 - accuracy: 0.7705\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1364 - accuracy: 0.7708\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1362 - accuracy: 0.7711\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1360 - accuracy: 0.7714\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1359 - accuracy: 0.7717\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1357 - accuracy: 0.7721\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.1355 - accuracy: 0.7725\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1353 - accuracy: 0.7728\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.1351 - accuracy: 0.7731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14560116a190>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "labels_train = K.one_hot(Y_train, nb_classes+1)\n",
    "X_train_r = X_train.reshape((X_train.shape[0], X_train.shape[1], 120))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(10, 120)))\n",
    "\n",
    "#output_dim is a categorical variable with 8 classes\n",
    "model.add(Dense(activation='softmax', units=3))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_r, labels_train, epochs=200, batch_size=None, steps_per_epoch=10, verbose=1, sample_weight=F_train, validation_split=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_r = X_val.reshape((X_val.shape[0], X_val.shape[1], 120))\n",
    "pred = model.predict(X_val_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10338772 0.6311512  0.26546106]\n",
      "Recall tf.Tensor(0.49328536, shape=(), dtype=float32)\n",
      "Precision tf.Tensor(0.7043925, shape=(), dtype=float32)\n",
      "F1 tf.Tensor(0.5802336, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(pred[0])\n",
    "labels_val = K.one_hot(Y_val, nb_classes+1)\n",
    "r = tf.keras.metrics.Recall()\n",
    "r.update_state(labels_val, pred)\n",
    "r.result().numpy()\n",
    "print('Recall ' + str(r.result()))\n",
    "\n",
    "p = tf.keras.metrics.Precision()\n",
    "p.update_state(labels_val, pred)\n",
    "p.result().numpy()\n",
    "print('Precision ' + str(p.result()))\n",
    "\n",
    "f1 = 2*((p.result()*r.result())/(p.result()+r.result()+K.epsilon()))\n",
    "print('F1 ' + str(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If your data is in the form of symbolic tensors, you should specify the `steps` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6ee2aad2d992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                                          \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m                                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    370\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                                     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                                     steps_name='steps')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;31m# Check if callbacks have not been already configured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoofing_new/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_num_samples\u001b[0;34m(ins, batch_size, steps, steps_name)\u001b[0m\n\u001b[1;32m    569\u001b[0m             raise ValueError(\n\u001b[1;32m    570\u001b[0m                 \u001b[0;34m'If your data is in the form of symbolic tensors, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 \u001b[0;34m'you should specify the `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'` argument '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;34m'(instead of the `batch_size` argument, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;34m'because symbolic tensors are expected to produce '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If your data is in the form of symbolic tensors, you should specify the `steps` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data)."
     ]
    }
   ],
   "source": [
    "hist = model.evaluate(X_val_r, labels_val)\n",
    "# summarize history for accuracy\n",
    "plt.plot(hist['accuracy'])\n",
    "plt.plot(hist['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spoofing_new]",
   "language": "python",
   "name": "conda-env-spoofing_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
